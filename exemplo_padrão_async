'''
-------------------------------------------------
SCRIPT PYTHON PARA EXTRAÇÃO DE DADOS DA DROGASIL.
-------------------------------------------------

Autor: bernardo.teixeira@polijunior.com.br
Data de Criação: 09/02/2024

Input: Não há input. O script é feito para ser rodado diretamente.
Output: Um arquivo .csv com os dados extraídos, salvo em 'drogasil_oficial.csv'. 

Metodologia utilizada: No site, existiam algumas informações que poderiam ser
extraídas pela API e outras que permitiam a extração por HTML. Por isso, o script
foi dividido em duas partes: uma para a extração por HTML e outra para a extração
por API. A extração por HTML foi feita de forma assíncrona, para otimizar o tempo
de execução. A extração por API foi feita de forma síncrona, pois a API não permitia
a extração de forma assíncrona. No fim, as informações extraídas por HTML e API
são unidas em um único dataframe.

Url do html: https://www.drogasil.com.br/saude.html?p=1 onde 'saude' é um dos departamentos
da drogasil. A partir dessa url, é possível acessar todas as páginas de produtos de um departamento.

Url da api: https://www.drogasil.com.br/api/next/middlewareGraphql

Payload da api: o payload é utilizado para obter as informações dos produtos
a partir de uma lista de SKUs. Essa lista foi determinada a partir dos produtos extraídos
pelo HTML. 
'''



import requests
import json
import pandas as pd
import re
from bs4 import BeautifulSoup
import asyncio
import aiohttp
import httpx
import aiometer
from functools import partial
import time
from bs4 import BeautifulSoup

async def get_responses(urls, tries = 10):
    responses = []
    async with httpx.AsyncClient() as client:
        sem = asyncio.Semaphore(5)
        async def requisition(url):
            async with sem:
                for i in range(tries):
                    try:
                        response = await client.get(url)
                        response.raise_for_status()
                        responses.append(response.text)
                        break
                    except httpx.HTTPError as e:
                        # Trata exceções específicas de HTTP
                        print(f"Erro durante a requisição: {e}")
                        print(url)
                    except Exception as e:
                        # Trata exceções genéricas
                        print(f"Erro desconhecido durante a requisição: {e}")
                        print(url)
        await asyncio.gather(*[requisition(url) for url in urls])
    return responses

async def main():
    print("------------------------")
    print("------------------------")
    print("------------------------")
    print("---INÍCIO DA EXTRAÇÃO---")
    print("------------------------")
    print("------------------------")
    print("------------------------")

    # Nome e quantidade de página dos departamentos:
    departamentos = ['mamae-e-bebe']
    paginas = [2]

    urls = []
    # Iterando por departamentos e formando URLS:
    for departamento, n_páginas in zip(departamentos, paginas):
        base_url = "https://www.drogasil.com.br/" + str(departamento) + ".html?p="
        urls_temp = [base_url + str(p) for p in range(1, n_páginas+1)]
        urls.extend(urls_temp)
    responses = await get_responses(urls)
    
    links = []
    for response in responses:
        links_temp = await extract_link(response)
        links.extend(links_temp)
    
    for link in links:
        responses = await get_responses(link)
    
    # Extraindo as informações de HTML
    i=1
    final_df = pd.DataFrame()
    for response in responses:
        df = await extract_info(response)  
        final_df = pd.concat([final_df,df], ignore_index=True)
        print("Response",i,"de",len(responses),"extraído.")
        i+=1

    
    final_df.to_csv('drogasil_oficial.csv', index=False)

    print("---------------------")
    print("---------------------")
    print("---------------------")
    print("---FIM DA EXTRAÇÃO---")
    print("---------------------")
    print("---------------------")
    print("---------------------")

async def extract_link(response):
    soup = BeautifulSoup(response, 'html.parser')
    # Encontre o script <script id="__NEXT_DATA__" type="application/json">:
    script = soup.find('script', id='__NEXT_DATA__')
    # Leia o script como json:
    script = json.loads(script.string)
    itens = script['props']['pageProps']['pageData']['items']
    dfs = []
    # Obtenha os dados importantes:
    for item in itens:
        sku = item['sku']
        name = item['name']
        custom = item['custom_attributes']
        for c in custom:
            if c['attribute_code'] == 'fabricante':
                fabricante = c['value'][0]['label']
            if c['attribute_code'] == 'marca':
                marca = c['value'][0]['label']
            if c['attribute_code'] == 'ean':
                ean = c['value_string'][0]
            if c['attribute_code'] == 'description':
                descricao = c['value_string']
        nova_linha = {'sku': sku, 'name': name, 'fabricante': fabricante, 'marca': marca, 'ean': ean, 'descricao': descricao}
        # Verifica se a nova_linha está completa:
        if nova_linha:
            df_temp = pd.DataFrame([nova_linha])
            dfs.append(df_temp)

    df_final = pd.DataFrame()
    # Apague dataframes nulos em dfs:
    dfs = [df for df in dfs if not df.empty]
    if dfs:
        df_final = pd.concat(dfs, ignore_index=True)
    return df_final

async def extract_info(api):
    api = api[0]
    itens = api['data']['productsBySkuList']
    dfs = []
    # Obtendo dados importantes:
    for item in itens:
        if item:
            sku = item['sku']
            if item['liveComposition'] and item['liveComposition']['livePrice'] and item['liveComposition']['livePrice']['bestPrice']:
                value_from = item['liveComposition']['livePrice']['bestPrice']['valueFrom']
            else:
                value_from = 'sem value_from'
            if item['liveComposition'] and item['liveComposition']['livePrice'] and item['liveComposition']['livePrice']['bestPrice']:
                value_to = item['liveComposition']['livePrice']['bestPrice']['valueTo']
            else:
                value_to = 'sem value_to'
            if item['liveComposition'] and item['liveComposition']['livePrice'] and item['liveComposition']['livePrice']['bestPrice']:
                value_type = item['liveComposition']['livePrice']['bestPrice']['type']
            else:
                value_type = 'sem value_type'
            if item['liveComposition'] and item['liveComposition']['livePrice'] and item['liveComposition']['livePrice']['bestPrice']:
                discountPercentage = item['liveComposition']['livePrice']['bestPrice']['discountPercentage'] if 'discountPercentage' in item['liveComposition']['livePrice']['bestPrice'] else None
            else:
                discountPercentage = 'sem discountPercentage'
            nova_linha = {'sku': sku, 'value_from': value_from, 'value_to': value_to, 'value_type': value_type, 'discountPercentage': discountPercentage}
            df_temp = pd.DataFrame([nova_linha])  # Criando DataFrame temporário
            dfs.append(df_temp)

    # Concatenando todos os DataFrames temporários
    df_final = pd.concat(dfs, ignore_index=True)
    
    return df_final
    
if __name__ == "__main__":
    asyncio.run(main())

