{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraindo os links dos produtos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorias = ['oncologia', 'medicamentos-especiais', 'imunologia']\n",
    "url_base = 'https://www.mundialfarma.com.br'\n",
    "workers = os.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "lista_links = []\n",
    "\n",
    "def extrair_links_pagina(categoria, pagina):\n",
    "    url = f'{url_base}/{categoria}?page={pagina}'\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        links = [a['href'] for a in soup.find_all('a', {'class': 'vtex-product-summary-2-x-clearLink h-100 flex flex-column'})]\n",
    "        return links\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "def extrair_links_todas_paginas():\n",
    "    with ThreadPoolExecutor(max_workers=workers) as executor:\n",
    "        for categoria in categorias:\n",
    "            for pagina in range(1, 51):\n",
    "                print(pagina)\n",
    "                arguments = (categoria, pagina)\n",
    "                links_por_pagina = list(executor.map(lambda args: extrair_links_pagina(*args), [arguments]))\n",
    "\n",
    "                if not links_por_pagina[0]:\n",
    "                    break\n",
    "\n",
    "                lista_links.extend(links_por_pagina[0])\n",
    "\n",
    "extrair_links_todas_paginas()\n",
    "\n",
    "lista_links_completa = [f'{url_base}{link}' for link in lista_links]\n",
    "df = pd.DataFrame({'URL': lista_links_completa})\n",
    "df_links = df.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "df_links.to_csv('df_links.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraindo as informações a partir de cada link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_info(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Encontre todas as tags script\n",
    "        script_tags = soup.find_all('script')\n",
    "\n",
    "        # Procurar a string '__RUNTIME__' em cada tag script\n",
    "        for script_tag in script_tags:\n",
    "            if '__RUNTIME__' in script_tag.string:\n",
    "                # Use expressões regulares para extrair informações adicionais\n",
    "                match_ean = re.search(r'\"ean\"\\s*:\\s*\"(.*?)\"', script_tag.string)\n",
    "                match_name = re.search(r'\"productName\"\\s*:\\s*\"(.*?)\"', script_tag.string)\n",
    "                match_description = re.search(r'\"description\"\\s*:\\s*\"(.*?)\"', script_tag.string)\n",
    "                match_price_c = re.search(r'\"Price\"\\s*:\\s*([\\d.]+)', script_tag.string)\n",
    "                match_price_s = re.search(r'\"ListPrice\"\\s*:\\s*([\\d.]+)', script_tag.string)\n",
    "                match_lab = re.search(r'\"name\"\\s*:\\s*\"Laboratório\",\"values\"\\s*:\\s*{\"type\":\"json\",\"json\":\\[\"(.*?)\"\\]', script_tag.string)\n",
    "\n",
    "                # Extrair valores correspondentes se encontrados\n",
    "                ean = match_ean.group(1) if match_ean else None\n",
    "                name = match_name.group(1) if match_name else None\n",
    "                description = match_description.group(1) if match_description else None\n",
    "                price_c = float(match_price.group(1)) if match_price_c else None\n",
    "                price_s = float(match_price.group(1)) if match_price_s else None\n",
    "\n",
    "                if price_c == price_s:\n",
    "                    desconto = 0\n",
    "                else:\n",
    "                    desconto = round(((price_s - price_c) / price_s) * 100)\n",
    "\n",
    "                lab = match_lab.group(1) if match_lab else None\n",
    "\n",
    "                info = {'Produto': name, 'EAN': ean, 'Marca': lab, 'Descrição': description, \n",
    "                        'Preço sem desconto': price_s, 'Preço com desconto': price_c, \n",
    "                        '% de desconto': desconto, 'Farmácia': 'Mundial Farma', 'Região': 'Sudeste', 'Cidade': 'São Paulo'}\n",
    "                return info\n",
    "        else:\n",
    "            print(f\"Não foi encontrado '__RUNTIME__' em nenhuma tag script para {url}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f'Erro ao processar a URL {url}: {e}')\n",
    "        return None\n",
    "\n",
    "infos = []\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=5) as executor:  # Ajuste o número de workers conforme necessário\n",
    "    results = executor.map(extract_info, df_links['URL'])\n",
    "\n",
    "    for result in results:\n",
    "        if result:\n",
    "            infos.append(result)\n",
    "\n",
    "df_infos = pd.DataFrame(infos)\n",
    "df_infos.to_csv('df_infos.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
